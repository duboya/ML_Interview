# GBDT 与 LR 的区别

## 1. 从机器学习三要素的角度：
### 1.1 模型
本质上来说，他们都是监督学习，判别模型，直接对数据的分布建模，不尝试挖据隐含变量，这些方面是大体相同的。但是又因为一个是线性模型，一个是非线性模型，因此其具体模型的结构导致了VC维的不同：
其中，Logistic Regression作为线性分类器，它的VC维是d+1，而 GBDT 作为boosting模型，可以无限分裂，具有无限逼近样本VC维的特点，因此其VC维远远大于d+1，这都是由于其线性分类器的特征决定的，归结起来，是Logistic Regression对数据线性可分的假设导致的

### 1.2 策略
从 Loss(经验风险最小化) + 正则(结构风险最小化) 的框架开始说起；

**从Loss的角度：**

因为 Logistic Regression 的输出是 y = 1 的概率，所以在极大似然下，Logistic Regression的Loss是交叉熵，此时，Logistic Regression的准则是最大熵原理，也就是“为了追求最小分类误差，追求最大熵Loss”，**本质上是分类器算法，而且对数据的噪声具有高斯假设**；
而 GBDT 采用 CART 树作为基分类器，其无论是处理分类还是回归均是将采用回归拟合（将分类问题通过 softmax 转换为回归问题，具体可参考本博客 GBDT 章节），用当前轮 CART 树拟合前一轮目标函数与实际值的负梯度：$h_t = -g​$，**本质上是回归算法**。

> 也正是因为 GBDT 采用的 CART 树模型作为基分类器进行负梯度拟合，其是一种对特征样本空间进行划分的策略，不能使用 SGD 等梯度优化算法，而是 CART 树自身的节点分裂策略：均方差(回归) 也带来了算法上的不同；
> GBDT 损失函数值得是前一轮拟合模型与实际值的差异，而树节点内部分裂的特征选择则是固定为 CART 的均方差，目标损失函数可以自定义，当前轮 CART 树旨在拟合负梯度。



**从特征空间的角度:**
就是因为 Logistic Regression 是特征的线性组合求交叉熵的最小化，也就是对特征的线性组合做 logistic，使得Logistic Regression会在特征空间中做线性分界面，适用于分类任务；

而 GBDT 采用 CART 树作为基分类器，其每轮树的特征拟合都是对特征空间做平行于坐标轴的空间分割，所以自带特征选择和可解释性，GBDT 即可处理分类问题也可解决回归问题，只是其统一采用回归思路进行求解（试想，如果不将分类转换为回归问题，GBDT 每轮目标函数旨在拟合上一轮组合模型的负梯度，分类信息无法求梯度，故而依旧是采用 softmax 转换为回归问题进行求解）。

> **线性分类器**（处理线性可分）有三大类：**感知器准则函数、SVM、Fisher准则**。
>
> **感知准则函数** ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。
>
> **支持向量机** ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）
>
> **Fisher 准则** ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条原点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。

**从正则的角度：**

Logistic Regression 的正则采用一种约束参数稀疏的方式，其中 L2 正则整体约束权重系数的均方和，使得权重分布更均匀，而 L1 正则则是约束权重系数绝对值和，其自带特征选择特性；

GBDT 的正则：

- 弱算法的个数T，就是迭代T轮。T的大小就影响着算法的复杂度
- 步长（Shrinkage）在每一轮迭代中，原来采用$F_t({\bf x}) = F_{t-1}({\bf x}) + \alpha_th_t({\bf x}; {\bf w_t})$进行更新，可以加入步长v，使得一次不更新那么多：$F_t({\bf x}) = F_{t-1}({\bf x}) + v \ \alpha_th_t({\bf x}; {\bf w_t}); v\in(0,1]$

> XGBoost的正则是在 GBDT 的基础上又添加了是一棵树里面节点的个数，以及每个树叶子节点上面输出分数的 L2 模平方。

区别在于 LR 采用对特征系数进行整体的限定，GBDT 采用迭代的误差控制本轮参数的增长；

### 1.3 算法

Logistic Regression 若采用 SGB, Momentum, SGD with Nesterov Acceleration 等算法，只用到了一阶导数信息，若用 AdaGrad, AdaDelta / RMSProp, Adam, Nadam, 牛顿法则用到了二阶导数信息，

而GBDT 直接拟合上一轮组合函数的特梯度，只用到了一阶倒数信息，XGBoost 则是用到了二阶导数信息。

> SAG/SAGA等优化器在scikit-learn上可用，但是业界用得比较多的还是BGFS，L-BGFS等，个人认为是计算量的原因，Logistic Regression模型很快就可以收敛，在线性可分的空间中也不容易出现鞍点，而且一般用Logistic Regression模型的数据量都比较大，大到不能上更复杂的模型，所以优化方法一般都是往计算量小的方向做。



## 2. 从特征的角度：

### 2.1 特征组合：

如前所说，GBDT 特征选择方法采用最小化均方损失来寻找分裂特征及对应分裂点，所以自动会在当前根据特征 A 分裂的子树下寻求其他能使负梯度最小的其他特征 B，这样就自动具备寻求好的特征组合的性能，因此也能给出哪些特征比较重要（根据该特征被选作分裂特征的次数）

而 LR 只是一次性地寻求最大化熵的过程，对每一维的特征都假设独立，因此只具备对已有特征空间进行分割的能力，更不会对特征空间进行升维（特征组合）

### 2.2 特征的稀疏性：

如前所述，Logistic Regression不具有特征组合的能力，并假设特征各个维度独立，因此只具有线性分界面，实际应用中，多数特征之间有相关性，只有维度特别大的稀疏数据中特征才会近似独立，所以适合应用在特征稀疏的数据上。
而对于 GBDT，其更适合处理稠密特征，如 GBDT+LR 的Facebook论文中，对于连续型特征导入 GBDT 做特征组合来代替一部分手工特征工程，而对于 ID 类特征的做法往往是 one-hot 之后直接传入 LR，或者先 hash，再 one-hot 传入树中进行特征工程，而目前的主流做法是直接 one-hot + embedding 来将高维稀疏特征压缩为低纬稠密特征，也进一步引入了语意信息，有利于特征的表达。

## 3. 数据假设不同：


逻辑回归的**第一个**基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是 p,抛中为负面的概率是 1−p。在逻辑回归这个模型里面是假设 $h_\theta\left(x\right )$ 为样本为正的概率，$1 - h_\theta\left(x\right )$ 为样本为负的概率。那么整个模型可以描述为:
$$
h_\theta\left(x;\theta \right )=p
$$
逻辑回归的第二个假设是假设样本为正的概率是 :
$$
p=\frac{1}{1+e^{-\theta^{T} x}}
$$
所以逻辑回归的最终形式 :
$$
h_\theta\left(x;\theta \right )=\frac{1}{1+e^{-\theta^{T} x}}
$$

总结，Logistic Regression的数据分布假设：

> 1. 噪声是高斯分布的
> 2. 数据服从伯努利分布
> 3. 特征独立

而 GBDT 并未对数据做出上述假设。